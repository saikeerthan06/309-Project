{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNZp6TuDgklY"
      },
      "source": [
        "Install Dependencies:\n",
        "\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with hadoop and\n",
        "3.   Findspark (used to locate the spark in the system)\n",
        "\n",
        "> If you have issues with spark version, please upgrade to the latest version from [here](https://archive.apache.org/dist/spark/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UdUeGk5wAf5g"
      },
      "outputs": [],
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3-scala2.13.tgz\n",
        "# #If there is an error please check the spark version in the link above\n",
        "# !tar xf spark-3.5.5-bin-hadoop3-scala2.13.tgz\n",
        "# #Please change the version accoding to the downloaded version\n",
        "# !pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HRyXyfvVg-jq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/local/opt/openjdk@8\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/Users/saikeerthan/spark\"#Please change the version accoding to the downloaded version\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkAt6vv_jOht"
      },
      "source": [
        "# Initialize a SparkSession\n",
        "\n",
        "`import findspark` and initialize it. This helps locate Spark in your system.\n",
        "\n",
        "Next, you create a SparkSession using this flow:\n",
        "\n",
        "`master(\"local[*]\")` configures Spark to run locally using all available cores.\n",
        "\n",
        "`appName(\"YourAppName\") `sets the name of your Spark application, which can be helpful for identification.\n",
        "\n",
        "`getOrCreate()` ensures that only one SparkSession is created in your application, retrieving the existing one if it exists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0l_vbe5_jOzF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/04/23 11:51:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "#Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"YourAppName\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMl0wXvXrfu9"
      },
      "source": [
        "# What is RDD\n",
        "RDD stands for Resilient Distributed Dataset.\n",
        "It is a fundamental data structure in Spark that represents an immutable, partitioned collection of elements that can be operated on in parallel.\n",
        "\n",
        "\n",
        "\n",
        "*   Resilient: RDDs are fault-tolerant, meaning that if a partition is lost, it can be reconstructed from the original data.\n",
        "*   Distributed: RDDs are distributed across multiple nodes in a cluster, enabling parallel processing.\n",
        "*  Dataset: RDDs store a collection of elements, which can be of any type.\n",
        "\n",
        "\n",
        "## Paralezing Data\n",
        "\n",
        "Parallelizing data means distributing the data across multiple nodes in a cluster to perform operations concurrently.\n",
        "\n",
        "This allows for faster processing of large datasets, as each node can work on a subset of the data simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nbsYSqKkA8Gh"
      },
      "outputs": [],
      "source": [
        "# Create RDD from parallelize\n",
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd=spark.sparkContext.parallelize(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y3P2-I4rcxQ"
      },
      "source": [
        "## What are partitions?\n",
        "A partition in spark is an atomic chunk of data (a logical division of data) stored on a node in the cluster.  \n",
        "\n",
        "Imagine: You are given 100kg of rice and it is stored in a silo. You are required to move it from point A to B (Single threaded resource).\n",
        "You can either move it from point A to B in one go however you do not have enough capacity (Not enough CPU or RAM). So instead you split it up into 5 kg bags of rice ( Pratition it into smaller chunks) and have more people to do it for you (Scaling up your cluster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FazxmcryA83V",
        "outputId": "04fba7d2-fd0a-4fe6-a94e-7334c31212c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial partition count:8\n"
          ]
        }
      ],
      "source": [
        "print(\"initial partition count:\"+str(rdd.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osvm95dsBUs_",
        "outputId": "9b36245c-4647-48a7-945c-a309969d2a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial partition count:6\n"
          ]
        }
      ],
      "source": [
        "#Determine the number of paritions required\n",
        "rdd=spark.sparkContext.parallelize([1,2,3,4,56,7,8,9,12,3],6)\n",
        "print(\"initial partition count:\"+str(rdd.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z202im3ABZbG",
        "outputId": "b0d3aaae-c3ea-41fc-829a-95c7aa9e6f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial partition count:6\n"
          ]
        }
      ],
      "source": [
        "print(\"initial partition count:\"+str(rdd.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P4nAZkKMBcSR"
      },
      "outputs": [],
      "source": [
        "#Create empty RDD with partition\n",
        "rdd2 = spark.sparkContext.parallelize([],11) #This creates 11 partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IiKu6WD8Beli"
      },
      "outputs": [],
      "source": [
        "# Creates empty RDD with no partition\n",
        "rdd = spark.sparkContext.emptyRDD\n",
        "# rddString = spark.sparkContext.emptyRDD[String]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TY7BRpkTBqfL"
      },
      "outputs": [],
      "source": [
        " rdd=spark.sparkContext.parallelize([1,2,3,4,56,7,8,9,12,3], 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjNGErICBsmb",
        "outputId": "42e2997e-d271-412e-b71c-3a0f5df7f61d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "re-partition count:4\n"
          ]
        }
      ],
      "source": [
        "reparRdd = rdd.repartition(4)\n",
        "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sdSkF51tdi-"
      },
      "source": [
        "# PySpark Dataframe\n",
        "In PySpark, a DataFrame is a distributed collection of data organized into named columns. It resembles a table in a relational database or a spreadsheet in which data is arranged in rows and columns. Each column can have a different data type, and DataFrames are highly optimized for parallel processing, making them suitable for handling large-scale datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Neg7AhEhCA2W",
        "outputId": "31177a77-012e-449b-a29d-b504d2e86735"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 11:52:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 34|\n",
            "|    Bob| 45|\n",
            "|Charlie| 29|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a list of tuples\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\n",
        "\n",
        "# Create a DataFrame from the list with column names\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axNpci27CIWF",
        "outputId": "694b3e02-2a58-4998-bb80-713eb4c30faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter the DataFrame to select rows where id is greater than 1\n",
        "filtered_df = df.filter(df['Age'] > 45)\n",
        "filtered_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7YnV5W9CMjT",
        "outputId": "c4265a8e-b834-4596-fb11-51dfb5d200df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 34|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Run a SQL query\n",
        "result = spark.sql(\"SELECT * FROM people WHERE Age = 34\")\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Q0KOM1mJCOeO"
      },
      "outputs": [],
      "source": [
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7zjyTyaCQh0",
        "outputId": "2d30fdb7-2a0f-4dff-dac4-90206ce6dd8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl8fOFlquvdy"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "With the data here: 4,6,68,23,7,7,3,1,8,9,20,1,5,7\n",
        "How do you create a RDD with 5 partition?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "FQbPiTsEux7K",
        "outputId": "aa7c4c2a-0441-4d03-e0f0-731fea792674"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/28 10:39:19 WARN Utils: Your hostname, MacBook-Air-8.local resolves to a loopback address: 127.0.0.1; using 172.27.159.167 instead (on interface en0)\n",
            "25/04/28 10:39:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/04/28 10:39:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of partitions: 5\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SimpleRDDExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "data = [4, 6, 68, 23, 7, 7, 3, 1, 8, 9, 20, 1, 5, 7]\n",
        "rdd = spark.sparkContext.parallelize(data, 5)  # Create an RDD with 5 partitions\n",
        "\n",
        "print(\"Number of partitions:\", rdd.getNumPartitions())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IVPUqJ8wHuj"
      },
      "source": [
        "How do you print the data stored in each of the partition?\n",
        "\n",
        "(Hint look up on the function .glom() and .collect() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAupPbTcvZtV",
        "outputId": "afc76dab-a440-48c7-e3b3-09303214c458"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 0:>                                                          (0 + 5) / 5]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Partition 0: [4, 6]\n",
            "Partition 1: [68, 23]\n",
            "Partition 2: [7, 7, 3, 1]\n",
            "Partition 3: [8, 9]\n",
            "Partition 4: [20, 1, 5, 7]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "def show_partition(index, iterator):\n",
        "    yield f\"Partition {index}: {list(iterator)}\"\n",
        "\n",
        "\n",
        "partition_data = rdd.mapPartitionsWithIndex(show_partition).collect()\n",
        "\n",
        "\n",
        "for pdata in partition_data:\n",
        "    print(pdata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlmf86OiwvcF"
      },
      "source": [
        "Create an newsparksession called \"Datframe_parition\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qaVq9FFPwDsI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/28 10:42:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Datframe_partition\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFYrP6MCxtm-"
      },
      "source": [
        "Create a dataframe to store the following information\n",
        "\n",
        "*   id:1,2,3,4,5,6\n",
        "*   Name: La,Bu,Bu,Little,John,Pop\n",
        "*   Age: 23,45,33,44,11,1\n",
        "\n",
        "Show that the dataframe has being created properly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWh25yuHxr4y",
        "outputId": "500b71f5-4a20-44f0-f487-f6f5acae3a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+---+\n",
            "| id|  name|age|\n",
            "+---+------+---+\n",
            "|  1|    La| 23|\n",
            "|  2|    Bu| 45|\n",
            "|  3|    Bu| 33|\n",
            "|  4|Little| 44|\n",
            "|  5|  John| 11|\n",
            "|  6|   Pop|  1|\n",
            "+---+------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(1, \"La\", 23), (2, \"Bu\", 45), (3, \"Bu\", 33), (4, \"Little\", 44), (5, \"John\", 11), (6, \"Pop\", 1 )]\n",
        "\n",
        "df2 = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
        "\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4F5X-lcyx0S"
      },
      "source": [
        "Partition the data and store it into an RDD with the same number of ID.\n",
        "Print the things stored in the partition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSlaMeseyhDt",
        "outputId": "bf857d0c-1d6f-4736-b6fd-6c2ffc208c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of partitions: 6\n"
          ]
        }
      ],
      "source": [
        "rdd2 = spark.sparkContext.parallelize(data, 6)  # Create an RDD with 6 partitions\n",
        "print(\"Number of partitions:\", rdd2.getNumPartitions())\n",
        "\n",
        "def show_partition(index, iterator):\n",
        "    yield f\"Partition {index}: {list(iterator)}\"\n",
        "\n",
        "    partition_data = rdd2.mapPartitionsWithIndex(show_partition).collect()\n",
        "\n",
        "    for pdata in partition_data:\n",
        "        print(pdata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Partition 0: [(1, 'La', 23)]\n",
            "Partition 1: [(2, 'Bu', 45)]\n",
            "Partition 2: [(3, 'Bu', 33)]\n",
            "Partition 3: [(4, 'Little', 44)]\n",
            "Partition 4: [(5, 'John', 11)]\n",
            "Partition 5: [(6, 'Pop', 1)]\n"
          ]
        }
      ],
      "source": [
        "def show_partition(index, iterator):\n",
        "    yield f\"Partition {index}: {list(iterator)}\"\n",
        "\n",
        "# Apply the function\n",
        "partition_data = rdd2.mapPartitionsWithIndex(show_partition).collect()\n",
        "\n",
        "# Print each partition's data\n",
        "for pdata in partition_data:\n",
        "    print(pdata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pejLZS3a0eg3"
      },
      "source": [
        "# Further into Data Partitioning\n",
        "\n",
        "Data partitioning refers to the process of dividing a large dataset into smaller chunks or partitions, which can be processed concurrently. This is an important aspect of distributed computing, as it allows large datasets to be processed more efficiently by dividing the workload among multiple machines or processors.\n",
        "\n",
        "* Advantages of Data Partitioning :\n",
        "Improved performance: By dividing data into smaller partitions, it can be processed in parallel across multiple machines, leading to faster processing times and improved performance.\n",
        "* Scalability: Partitioning allows for horizontal scalability, meaning that as the amount of data grows, more machines can be added to the cluster to handle the increased load, without having to make changes to the data processing code.\n",
        "Improved fault tolerance: Partitioning also allows for data to be distributed across multiple machines, which can help to prevent data loss in the event of a single machine failure.\n",
        "* Data organization: Partitioning allows for data to be organized in a more meaningful way, such as by time period or geographic location, which can make it easier to analyze and query the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvpNFs3o0zAI"
      },
      "source": [
        "Methods of data partitioning in PySpark:\n",
        "1. Hash Partitioning\n",
        "2. Range Partitioning\n",
        "3. Using partitionBy\n",
        "\n",
        "What we have tired and done so far is hash partitioning, the most common and default way of partitioning data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GySc1N-O15Gp"
      },
      "source": [
        "# Range Partitioning\n",
        "\n",
        "It involves dividing the data into partitions based on a range of values for a specified column.\n",
        "\n",
        "For example, we could partition a dataset based on a range of dates, with each partition containing records from a specific time period.\n",
        "\n",
        "In this method, we will use the repartitionByRange() function to perform range partitioning on the DataFrame based on the age column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8H6weXB0w_K",
        "outputId": "aa06dabb-d06e-431c-c555-7d6d08a2124f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/28 10:50:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
            "[Stage 6:>                                                          (0 + 8) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[Row(id=3, name='Be', age=25), Row(id=4, name='Hangry', age=26)], [Row(id=2, name='Bu', age=33), Row(id=6, name='Boby', age=30)], [Row(id=1, name='Du', age=45), Row(id=5, name='Eve', age=43)]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"range_partitioning\").getOrCreate()\n",
        "\n",
        "# Create a sample DataFrame\n",
        "df = spark.createDataFrame([\n",
        "    (1, \"Du\", 45),\n",
        "    (2, \"Bu\", 33),\n",
        "    (3, \"Be\", 25),\n",
        "    (4, \"Hangry\", 26),\n",
        "    (5, \"Eve\", 43),\n",
        "    (6, \"Boby\", 30)\n",
        "], [\"id\", \"name\", \"age\"])\n",
        "\n",
        "# Perform range partitioning on the\n",
        "# DataFrame based on the \"age\" column\n",
        "df = df.repartitionByRange(3, \"age\")\n",
        "\n",
        "# Print the elements in each partition\n",
        "print(df.rdd.glom().collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzHjvPxR3ZFH"
      },
      "source": [
        "From the output above you can see the data frame is partitioned into three parts [] as specified in the repartitionByRange() function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmVfUwcW3usS"
      },
      "source": [
        "# Using partitionBy\n",
        "\n",
        "The partitionBy() method in PySpark is used to split a DataFrame into smaller, more manageable partitions based on the values in one or more columns. The method takes one or more column names as arguments and returns a new DataFrame that is partitioned based on the values in those column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em_a0W6k2Yty",
        "outputId": "bc139385-2a57-4d1f-c89a-c8a45f81a683"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/28 10:50:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----+-------+------------+\n",
            "|country|year|product|sales_amount|\n",
            "+-------+----+-------+------------+\n",
            "|    USA|2022| Laptop|        1000|\n",
            "|  India|2022| Laptop|        1200|\n",
            "|    USA|2023| Mobile|         700|\n",
            "|  India|2023| Mobile|         800|\n",
            "|     UK|2023| Laptop|        1100|\n",
            "+-------+----+-------+------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"PartitionByExample\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"USA\", 2022, \"Laptop\", 1000),\n",
        "    (\"India\", 2022, \"Laptop\", 1200),\n",
        "    (\"USA\", 2023, \"Mobile\", 700),\n",
        "    (\"India\", 2023, \"Mobile\", 800),\n",
        "    (\"UK\", 2023, \"Laptop\", 1100)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "columns = [\"country\", \"year\", \"product\", \"sales_amount\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Partition the DataFrame by 'country' and save it to a specific directory\n",
        "df.write.option(\"header\", True).partitionBy(\"country\").mode(\"overwrite\").csv(\"countr\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrrp8ZvwVnme"
      },
      "source": [
        "# Hash Partitioning\n",
        "\n",
        "This is another example of how hash partitioning can be done based on what we have gone through at the start.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0dqhKpVCVnV1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of partitions:  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 13:=====================>                                    (3 + 5) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Partitioned data:  [[(3, 'c')], [(1, 'a'), (4, 'd')], [(2, 'b'), (5, 'e')]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create a pair RDD (key-value RDD)\n",
        "data = [(1, \"a\"), (2, \"b\"), (3, \"c\"), (4, \"d\"), (5, \"e\")]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Partition the RDD by key using hash partitioning (3 partitions)\n",
        "partitioned_rdd = rdd.partitionBy(3)\n",
        "\n",
        "# Check the number of partitions\n",
        "print(\"Number of partitions: \", partitioned_rdd.getNumPartitions())\n",
        "\n",
        "# Collect data from the partitioned RDD\n",
        "print(\"Partitioned data: \", partitioned_rdd.glom().collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcmIN4MX4asX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Y3S1_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
