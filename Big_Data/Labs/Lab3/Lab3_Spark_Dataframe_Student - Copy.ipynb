{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU9z7URp1yTV"
      },
      "source": [
        "### Lab 3 Spark Dataframe Fundamentals\n",
        "\n",
        "In this lab we would be going through more in depth into Datafames.\n",
        "\n",
        "Spark dataframes are similiar to Pandas dataframe but there are some differences which we would be going through in this exercise.\n",
        "\n",
        "Some functions and things that would be introducted are:\n",
        "1. read()\n",
        "2. explain()\n",
        "3. printSchema()\n",
        "4. inferSchema()\n",
        "5. DoubleData Type\n",
        "6. Other Datattypes\n",
        "7. Data Transformation\n",
        "8. Data ETL using PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FieAdIY9C5PO"
      },
      "outputs": [],
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3-scala2.13.tgz\n",
        "# !tar xf spark-3.5.5-bin-hadoop3-scala2.13.tgz\n",
        "# !pip install -q findspark\n",
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3-scala2.13\"\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/local/opt/openjdk@8\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/Users/saikeerthan/spark\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T09:12:09.811036Z",
          "start_time": "2020-11-09T09:12:09.799035Z"
        },
        "id": "5Oc0MKjuCvdB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW5lFBuWCvdD"
      },
      "source": [
        "### Create a spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFkTzJgkCvdF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/13 12:35:15 WARN Utils: Your hostname, MacBook-Air-8.local resolves to a loopback address: 127.0.0.1; using 172.27.145.58 instead (on interface en0)\n",
            "25/05/13 12:35:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/05/13 12:35:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.config(\"spark.master\", 'local').getOrCreate()\n",
        "\n",
        "# SparkSession.builder is used to initiate a new Spark Session, which is the main entry point for the Spark Functionality. \n",
        "# .config(\"spark.master\", 'local') - used to set a configuration property, where Spark will run locsally using a single thread. \n",
        "# .getOrCreate() - will retrieve the spark session if it exists, otherwise ti will create a new session in the environment. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8DUWc6LCvdG"
      },
      "source": [
        "# Data Schema\n",
        "\n",
        "Brazilian E-Commerce Public Dataset by Olist\n",
        "\n",
        "https://www.kaggle.com/olistbr/brazilian-ecommerce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2konSISJCvdG"
      },
      "source": [
        "# What is read()\n",
        "Reading of data from CSV file and storing the information into a dataframe with headers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T02:50:02.484884Z",
          "start_time": "2020-11-09T02:50:01.978336Z"
        },
        "id": "dEMaufjmCvdH",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df = spark.read.option('header', 'true').csv('olist_order_items_dataset.csv')\n",
        "\n",
        "# .read - accesses the spark DataFrameReader, which is used to load data from sources like JSON, CSV etc. \n",
        "# .option('header', 'true') - tells Spark that the first row of the CSV file is a header and does not contain actual data. the standalone option is used to tell Spark details about the first row of the file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWRvgb_sCvdH",
        "outputId": "a2330366-773f-4d8a-a731-77b7396cd76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.90|        13.29|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.90|        19.93|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.00|        17.87|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18| 12.99|        12.79|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.90|        18.14|\n",
            "|00048cc3ae777c65d...|            1|ef92defde845ab845...|6426d21aca402a131...|2017-05-23 03:55:27| 21.90|        12.69|\n",
            "|00054e8431b9d7675...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-14 12:10:31| 19.90|        11.85|\n",
            "|000576fe39319847c...|            1|557d850972a7d6f79...|5996cddab893a4652...|2018-07-10 12:30:45|810.00|        70.75|\n",
            "|0005a1a1728c9d785...|            1|310ae3c140ff94b03...|a416b6a846a117243...|2018-03-26 18:31:29|145.95|        11.65|\n",
            "|0005f50442cb953dc...|            1|4535b0e1091c278df...|ba143b05f0110f0dc...|2018-07-06 14:10:56| 53.99|        11.40|\n",
            "|00061f2a7bc09da83...|            1|d63c1011f49d98b97...|cc419e0650a3c5ba7...|2018-03-29 22:28:09| 59.99|         8.88|\n",
            "|00063b381e2406b52...|            1|f177554ea93259a5b...|8602a61d680a10a82...|2018-07-31 17:30:39| 45.00|        12.98|\n",
            "|0006ec9db01a64e59...|            1|99a4788cb24856965...|4a3ca9315b744ce9f...|2018-07-26 17:24:20| 74.00|        23.32|\n",
            "|0008288aa423d2a3f...|            1|368c6c730842d7801...|1f50f920176fa81da...|2018-02-21 02:55:52| 49.90|        13.37|\n",
            "|0008288aa423d2a3f...|            2|368c6c730842d7801...|1f50f920176fa81da...|2018-02-21 02:55:52| 49.90|        13.37|\n",
            "|0009792311464db53...|            1|8cab8abac59158715...|530ec6109d11eaaf8...|2018-08-17 12:15:10| 99.90|        27.65|\n",
            "|0009c9a17f916a706...|            1|3f27ac8e699df3d30...|fcb5ace8bcc92f757...|2018-05-02 09:31:53|639.00|        11.34|\n",
            "|000aed2e25dbad2f9...|            1|4fa33915031a8cde0...|fe2032dab1a61af87...|2018-05-16 20:57:03|144.00|         8.77|\n",
            "|000c3e6612759851c...|            1|b50c950aba0dcead2...|218d46b86c1881d02...|2017-08-21 03:33:13| 99.00|        13.71|\n",
            "|000e562887b1f2006...|            1|5ed9eaf534f6936b5...|8cbac7e12637ed9cf...|2018-02-28 12:08:37| 25.00|        16.11|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Djc1MotWKcm"
      },
      "source": [
        "# What is explain()\n",
        "**explain()**\n",
        "\n",
        "The `.explain()` method in Spark provides insights into the execution plan of a DataFrame or RDD operation. It is a powerful tool for understanding how Spark will execute a given operation, making it useful for debugging and query optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yNM-0jKCvdI",
        "outputId": "9b77bd2e-80d9-4c70-a885-3abd8507f724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "FileScan csv [order_id#17,order_item_id#18,product_id#19,seller_id#20,shipping_limit_date#21,price#22,freight_value#23] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/saikeerthan/NYP-AI/Year3/Big_Data/Labs/Lab3/olist_order_it..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,order_item_id:string,product_id:string,seller_id:string,shipping_limit_dat...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.explain()\n",
        "\n",
        "# .explain() - provides detialed insights about the execution plan of the dataframe, understanding how Spark will execute the given operation which is useful for debugging and performance tuning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hweU3--7CvdJ"
      },
      "source": [
        "#What is printSchema()\n",
        "\n",
        "## Loading data with a proper schema\n",
        "\n",
        "`df.printSchema()` lists out the data type of each column. Directly loading our csv file into Spark creates a dataframe where all columns are set to String types. While, this is not ideal as we prefer to have numeric types for `price` and datetime types for `date`.\n",
        "\n",
        "More information of data types supported can be found here:\n",
        "\n",
        "https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
        "\n",
        "```\n",
        "root\n",
        " |-- order_id: string (nullable = true)\n",
        " |-- order_item_id: string (nullable = true)\n",
        " |-- product_id: string (nullable = true)\n",
        " |-- seller_id: string (nullable = true)\n",
        " |-- shipping_limit_date: string (nullable = true)\n",
        " |-- price: string (nullable = true)\n",
        " |-- freight_value: string (nullable = true)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T03:46:54.973403Z",
          "start_time": "2020-11-09T03:46:54.968702Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-7FUC5mCvdJ",
        "outputId": "80fc10c1-af09-48df-a58b-c1c4e58d5d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_item_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- seller_id: string (nullable = true)\n",
            " |-- shipping_limit_date: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- freight_value: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()\n",
        "\n",
        "# nullable = true indicates that the column contains null values, however for now, it only infers them as strings and in order to display the correct data types, we need to use inferSchema. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T01:43:24.983895Z",
          "start_time": "2020-11-09T01:43:24.981412Z"
        },
        "id": "LnT8HmRYCvdK"
      },
      "source": [
        "### Let Spark to infer the schema\n",
        "\n",
        "We can use the `inferSchema` option to ask Spark to guess the best suitable schema of the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T01:42:52.226975Z",
          "start_time": "2020-11-09T01:42:49.923596Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtmfN9YiCvdK",
        "outputId": "2e7b7749-40fe-4b9e-b671-b7d8582cdaed",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_item_id: integer (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- seller_id: string (nullable = true)\n",
            " |-- shipping_limit_date: timestamp (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- freight_value: double (nullable = true)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_infer = spark.read.option('header', 'true').option('inferSchema', 'true').csv('olist_order_items_dataset.csv')\n",
        "df_infer.printSchema()\n",
        "\n",
        "# \"option('inferSchema', 'true')\" - tells SPark to automatically detect the data types based on the content of each column and if not for this option Spark will teat all columsn as strings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtzbwGQ2beYR"
      },
      "source": [
        "## Double Data Type\n",
        "\n",
        "DoubleType : Represents 64-byte double-precision floating point numbers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmndH4MxCvdL"
      },
      "source": [
        "DoubleType (from the pyspark.sql.types module) represents the data type for double-precision floating-point numbers. These numbers can hold a wider range of values compared to single-precision floats (FloatType), making them suitable for storing data that requires more precision, such as scientific calculations, financial data, or coordinates.\n",
        "\n",
        "\n",
        "*   Representation: Double-precision floats typically use 64 bits, allowing for a wider range and higher precision than single-precision floats (32 bits).\n",
        "\n",
        "*   Use Cases: Ideal for data requiring high accuracy, like scientific computations, financial calculations (percentages, interest rates), or coordinates (latitude, longitude).\n",
        "\n",
        "* Examples:Scientific constants (e.g., pi, Avogadro's number), currency values, geographical coordinates.\n",
        "\n",
        "#### When to Use DoubleType:\n",
        "\n",
        "When you need to store numbers with a wider range or higher precision than single-precision floats can provide.\n",
        "When dealing with calculations that require accurate results, such as scientific or financial computations.\n",
        "When the data inherently represents continuous values with a high degree of precision (e.g., scientific measurements, financial data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sIovEpUCvdK"
      },
      "source": [
        "# Using a user-specified schema\n",
        "\n",
        "If there is a fixed schema that you would want you can specified it also"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T01:50:59.821022Z",
          "start_time": "2020-11-09T01:50:59.744520Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf96siPGCvdL",
        "outputId": "69e9a5fd-ab02-4786-c3f8-e51a0e14f93a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_item_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- seller_id: string (nullable = true)\n",
            " |-- shipping_limit_date: timestamp (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- freight_value: integer (nullable = true)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/13 13:09:46 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 259230 ms exceeds timeout 120000 ms\n",
            "25/05/13 13:09:46 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
            "25/05/13 13:09:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:09:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:11:03 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:11:03 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:11:13 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:11:13 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:11:43 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:11:43 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:13:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:13:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:14:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:14:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:16:02 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:16:02 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:16:12 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:16:12 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:18:13 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:18:13 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:31:49 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:31:49 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:32:05 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:32:05 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:40:35 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:40:35 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:55:59 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:55:59 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 13:56:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 13:56:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 14:12:46 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 14:12:46 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 14:28:45 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 14:28:45 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 14:32:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 14:32:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 14:32:28 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 14:32:28 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:01:19 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:01:19 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:05:00 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:05:00 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:05:10 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:05:10 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:20:34 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:20:34 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:36:44 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:36:44 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:46:50 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:46:50 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:47:00 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:47:00 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:47:10 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:47:10 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:47:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:47:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:49:59 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:49:59 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:50:09 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:50:09 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:50:52 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:50:52 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:51:02 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:51:02 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:51:56 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:51:56 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:52:06 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:52:06 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:52:16 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:52:16 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:52:26 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:52:26 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:52:36 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:52:36 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:53:03 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:53:03 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:53:13 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:53:13 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:53:29 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:53:29 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 15:53:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 15:53:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:02:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:02:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:02:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:02:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:25:24 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:25:24 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:25:34 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:25:34 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:26:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:26:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:26:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:26:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:27:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:27:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:27:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:27:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:27:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:27:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:27:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:27:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:27:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:27:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:27:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:27:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:28:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:28:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:28:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:28:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:28:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:28:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/13 16:28:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:28:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:28:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:28:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:28:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:28:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:29:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:29:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:29:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:29:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.27.145.58:52842\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
            "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
            "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/13 16:29:17 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StringType, TimestampType, DoubleType,IntegerType\n",
        "\n",
        "my_schema = StructType() \\\n",
        "    .add(\"order_id\",StringType(),True) \\\n",
        "    .add(\"order_item_id\",StringType(),True) \\\n",
        "    .add(\"product_id\",StringType(),True) \\\n",
        "    .add(\"seller_id\",StringType(),True) \\\n",
        "    .add(\"shipping_limit_date\",TimestampType(),True) \\\n",
        "    .add(\"price\",DoubleType(),True) \\\n",
        "    .add(\"freight_value\",IntegerType(),True)\n",
        "\n",
        "# the above code is sued to build a custom scheme for the Dataframe with specific column names and it's specified data types. \"True\" as the third argument translates the to the column having null values. \n",
        "\n",
        "# the below code is used to load the CSV into the custom schema defined above. \n",
        "df_user = spark.read \\\n",
        "    .option('header', 'true') \\\n",
        "    .schema(my_schema) \\\n",
        "    .csv('olist_order_items_dataset.csv') # the CSV file is loaded into the DataFrame with the custom schema.\n",
        "\n",
        "df_user.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYPp8JHQdJjj"
      },
      "source": [
        "# Different Data Types\n",
        "\n",
        "More information can be found here:\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtkKZzaLdVDO",
        "outputId": "2104a87d-8676-4c6e-b0fa-9c61291d753e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_item_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- seller_id: string (nullable = true)\n",
            " |-- shipping_limit_date: timestamp (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- freight_value: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StringType, TimestampType, DoubleType,IntegerType\n",
        "\n",
        "my_schema = StructType() \\\n",
        "    .add(\"order_id\",StringType(),True) \\\n",
        "    .add(\"order_item_id\",StringType(),True) \\\n",
        "    .add(\"product_id\",StringType(),True) \\\n",
        "    .add(\"seller_id\",StringType(),True) \\\n",
        "    .add(\"shipping_limit_date\",TimestampType(),True) \\\n",
        "    .add(\"price\",DoubleType(),True) \\\n",
        "    .add(\"freight_value\",DoubleType(),True)\n",
        "\n",
        "df_user = spark.read \\\n",
        "    .option('header', 'true') \\\n",
        "    .schema(my_schema) \\\n",
        "    .csv('olist_order_items_dataset.csv')\n",
        "\n",
        "df_user.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlyXErtyCvdM"
      },
      "source": [
        " # Data transform based on columns' schema\n",
        "\n",
        " A right schema makes it much easier to process our data as we can utilize native functions Spark provides to supported data types. Check [Spark documentation](https://spark.apache.org/docs/2.3.0/api/sql/index.html)  to learn more about available functions.\n",
        "\n",
        " For example, we can extract `year` and `month` information from the `shipping_limit_date` column and add them as expanded columns in our dataframe.\n",
        "\n",
        " In Spark, adding a new column is done by calling the `withColumn` interface of a dataframe:\n",
        "\n",
        " ```\n",
        "df_user1 = df_user.withColumn('Y', year('shipping_limit_date'))\n",
        "df_user1.show(5)\n",
        "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+----+\n",
        "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|   Y|\n",
        "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+----+\n",
        "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35|  58.9|        13.29|2017|\n",
        "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13| 239.9|        19.93|2017|\n",
        "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30| 199.0|        17.87|2018|\n",
        "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18| 12.99|        12.79|2018|\n",
        "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51| 199.9|        18.14|2017|\n",
        "\n",
        " ```\n",
        "\n",
        " If we need to add multiple columns, we can pipe a seriese of `withColumn` calls together:\n",
        " ```\n",
        " df_user1 = df_user \\\n",
        "    .withColumn('Y', year('shipping_limit_date')) \\\n",
        "    .withColumn('M', month('shipping_limit_date'))\n",
        " ```\n",
        "\n",
        " __Note__: Spark does not allow you to modify an existing dataframe in-place. Every transform creates a new dataframe, and this is why we always assign a new dataframe to the transformed result `df_user1 = df_user.withColumn(...)`. But don't worry such a behavior might waste your ram -- Spark internally maintains linkage among transformed dataframes and minize memory usage automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T03:43:36.608858Z",
          "start_time": "2020-11-09T03:43:36.378410Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWTqMviiCvdM",
        "outputId": "d641e27e-1c8f-4e9d-b373-c8a2002ca354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|   Y|  M|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|2017|  9|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|2017|  5|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|2018|  1|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|2018|  8|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|2017|  2|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import year, month,days\n",
        "\n",
        "df_user1 = df_user \\\n",
        "    .withColumn('Y', year('shipping_limit_date')) \\\n",
        "    .withColumn('M', month('shipping_limit_date'))\n",
        "df_user1.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUh8JkFJdtMb"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "Can you extract the day and time from the shipping limit date?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlBuZYgUdsui",
        "outputId": "157cb62e-2c83-44ab-e410-4a90ad70c050"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|   Y|  M|  D|  H|Min|  S|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|2017|  9| 19|  9| 45| 35|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|2017|  5|  3| 11|  5| 13|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|2018|  1| 18| 14| 48| 30|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|2018|  8| 15| 10| 10| 18|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|2017|  2| 13| 13| 57| 51|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T03:44:56.241048Z",
          "start_time": "2020-11-09T03:44:56.238409Z"
        },
        "id": "yXKG1yaNCvdM"
      },
      "source": [
        "# Data ETL\n",
        "\n",
        "ETL refers to 'Extract, Transform, and Load', which is almost the first data requirement in any project. Spark dataframe make ETL easy as many typical operations are already abstracted and embedded into Spark dataframe interfaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T03:51:39.591448Z",
          "start_time": "2020-11-09T03:51:39.588577Z"
        },
        "id": "-CUZGGFrCvdN"
      },
      "source": [
        "## Filtering\n",
        "\n",
        "`Filtering` is the most common `Extract` operation, where we want to focus only on data that meets certain criterias.\n",
        "For example, if we want to look at orders in year 2017, we can use the following filter:\n",
        "\n",
        "```\n",
        "df_user1.filter(df_user1.Y == 2017)\n",
        "```\n",
        "\n",
        "Similarly, you can also apply conditions with `greater`, `less`, or `not equal` to filter dataframes.\n",
        "\n",
        "\n",
        "__Note__: `Y` column is of `integer` type. But if you use `df_user1.filter(df_user1.Y == \"2017\")`, you might be suprised that Spark still returns you the right result. This is becuase Spark trys to cast the column into a matching type to ensure the `comparison` is executable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T04:03:12.343288Z",
          "start_time": "2020-11-09T04:03:11.938767Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I33S8DUKCvdN",
        "outputId": "31ab69b4-e73d-4a3c-c0ec-3746b0bf4701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Compare with an integer\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|   Y|  M|  D|  H|Min|  S|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|2017|  9| 19|  9| 45| 35|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|2017|  5|  3| 11|  5| 13|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|2017|  2| 13| 13| 57| 51|\n",
            "|00048cc3ae777c65d...|            1|ef92defde845ab845...|6426d21aca402a131...|2017-05-23 03:55:27| 21.9|        12.69|2017|  5| 23|  3| 55| 27|\n",
            "|00054e8431b9d7675...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-14 12:10:31| 19.9|        11.85|2017| 12| 14| 12| 10| 31|\n",
            "|000c3e6612759851c...|            1|b50c950aba0dcead2...|218d46b86c1881d02...|2017-08-21 03:33:13| 99.0|        13.71|2017|  8| 21|  3| 33| 13|\n",
            "|000e906b789b55f64...|            1|57d79905de06d8897...|ea8482cd71df3c196...|2017-11-27 19:09:02|21.99|        11.85|2017| 11| 27| 19|  9|  2|\n",
            "|0010b2e5201cc5f1a...|            1|5a419dbf24a8c9718...|3504c0cb71d7fa48d...|2017-09-15 18:04:37| 48.9|         16.6|2017|  9| 15| 18|  4| 37|\n",
            "|00119ff934e539cf2...|            1|21b1c2f67a9aafb5a...|c864036feaab8c165...|2017-08-11 00:35:12|219.9|        16.98|2017|  8| 11|  0| 35| 12|\n",
            "|00125cb692d048878...|            1|1c0c0093a48f13ba7...|41b39e28db005d973...|2017-03-29 13:05:42|109.9|        25.51|2017|  3| 29| 13|  5| 42|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "only showing top 10 rows\n",
            "\n",
            "*** Compare with a string\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|   Y|  M|  D|  H|Min|  S|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|2017|  9| 19|  9| 45| 35|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|2017|  5|  3| 11|  5| 13|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"*** Compare with an integer\")\n",
        "df_user1.filter(df_user1.Y == 2017).show(10)\n",
        "\n",
        "print(\"*** Compare with a string\")\n",
        "df_user1.filter(df_user1.Y == \"2017\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwgqVcxlht6d",
        "outputId": "76f8fd3b-ee31-4839-9048-08ea93682c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+----+---+---+---+---+---+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|   Y|  M|  D|  H|Min|  S|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+----+---+---+---+---+---+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35|  58.9|        13.29|2017|  9| 19|  9| 45| 35|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13| 239.9|        19.93|2017|  5|  3| 11|  5| 13|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30| 199.0|        17.87|2018|  1| 18| 14| 48| 30|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18| 12.99|        12.79|2018|  8| 15| 10| 10| 18|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51| 199.9|        18.14|2017|  2| 13| 13| 57| 51|\n",
            "|00048cc3ae777c65d...|            1|ef92defde845ab845...|6426d21aca402a131...|2017-05-23 03:55:27|  21.9|        12.69|2017|  5| 23|  3| 55| 27|\n",
            "|00054e8431b9d7675...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-14 12:10:31|  19.9|        11.85|2017| 12| 14| 12| 10| 31|\n",
            "|000576fe39319847c...|            1|557d850972a7d6f79...|5996cddab893a4652...|2018-07-10 12:30:45| 810.0|        70.75|2018|  7| 10| 12| 30| 45|\n",
            "|0005a1a1728c9d785...|            1|310ae3c140ff94b03...|a416b6a846a117243...|2018-03-26 18:31:29|145.95|        11.65|2018|  3| 26| 18| 31| 29|\n",
            "|0005f50442cb953dc...|            1|4535b0e1091c278df...|ba143b05f0110f0dc...|2018-07-06 14:10:56| 53.99|         11.4|2018|  7|  6| 14| 10| 56|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+----+---+---+---+---+---+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|   Y|  M|  D|  H|Min|  S|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|2018|  1| 18| 14| 48| 30|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|2018|  8| 15| 10| 10| 18|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#### Take note if you just run the filter function it would not change your df that just have the filterd info. You need to store it into another dataframe\n",
        "df_user1.show(10)\n",
        "df_69=df_user1.filter(df_user1.Y == 2018)\n",
        "df_69.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uMaqfNqCvdN"
      },
      "source": [
        "For columns with `string` type, sometime we might need to filter them by prefixes, postfixes or particular patterns. Spark provides a few string functions to make it simple for us to express these logics.\n",
        "\n",
        "For example, if we want to work with 'order_ids' that starts with `\"2\"`, we can fitler the dataframe using the `substring` function:\n",
        "\n",
        "```\n",
        "df_user.filter(substring(df_user.order_id, 0, 1) == '2').show()\n",
        "```\n",
        "\n",
        "`substring` helps us extract a segment of the string based on the start and end location we provides.\n",
        "\n",
        "\n",
        "__<span style=\"color:red\">Quiz:</span>__: How to filter `order_id` start with `2002`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T04:22:25.861369Z",
          "start_time": "2020-11-09T04:22:24.248654Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWZQ09UHCvdN",
        "outputId": "8ea826cd-2ada-4871-9dcd-909a9e930d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "|20020569f2b852ebd...|            1|5c46bf87ac19e3912...|54965bbe3e4f07ae0...|2017-11-13 00:50:18| 89.9|        14.36|\n",
            "|2002527d89799a0cf...|            1|3b037eb584c1cf92f...|4324dd16853115efb...|2017-10-17 18:15:09|78.85|        35.25|\n",
            "|2002527d89799a0cf...|            2|e59dd207c69d86e89...|e9bc59e7b60fc3063...|2017-10-17 18:15:09|174.0|         0.21|\n",
            "|2002ea16e75277eaa...|            1|a184d57c818939252...|520b493b57809f446...|2017-07-26 22:30:10| 15.0|         9.94|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import substring\n",
        "\n",
        "df_user.filter(substring(df_user.order_id,0,4) == '2002').show(5)\n",
        "\n",
        "#(0,4) it is referring to the index position of the order_id numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWiUZiKDCvdO",
        "outputId": "733e3f84-d6b6-4739-d3c6-6247a5b525e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "|2002527d89799a0cf...|            1|3b037eb584c1cf92f...|4324dd16853115efb...|2017-10-17 18:15:09|78.85|        35.25|\n",
            "|2002527d89799a0cf...|            2|e59dd207c69d86e89...|e9bc59e7b60fc3063...|2017-10-17 18:15:09|174.0|         0.21|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_user.filter(substring(df_user.order_id,0,5) == '20025').show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_KtQeR-jeaK"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "How can I check if my database is there any information with price that is exactly $78.30?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia_cNY0ZjPJL",
        "outputId": "9d8ed41e-ba2f-455a-9b50-2e5d3eaa5546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
            "|order_id|order_item_id|product_id|seller_id|shipping_limit_date|price|freight_value|\n",
            "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
            "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUCe0FyGl5Kw"
      },
      "source": [
        "How about the range between 78.30 to 100?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzdgI9cUlLNF",
        "outputId": "fe458b83-b4a7-45e7-e333-e30cc64567f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|   Y|  M|  D|  H|Min|  S|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|0009792311464db53...|            1|8cab8abac59158715...|530ec6109d11eaaf8...|2018-08-17 12:15:10| 99.9|        27.65|2018|  8| 17| 12| 15| 10|\n",
            "|000c3e6612759851c...|            1|b50c950aba0dcead2...|218d46b86c1881d02...|2017-08-21 03:33:13| 99.0|        13.71|2017|  8| 21|  3| 33| 13|\n",
            "|001862358bf858722...|            1|c6dd917a0be2a7045...|7a67c85e85bb2ce85...|2018-02-13 19:10:37|99.99|        13.72|2018|  2| 13| 19| 10| 37|\n",
            "|001b76dd48a5b1eee...|            1|dbb67791e405873b2...|9c0e69c7bf2619675...|2018-04-02 17:50:17|81.99|        13.01|2018|  4|  2| 17| 50| 17|\n",
            "|001c85b5f68d2be0c...|            1|84f45695836516442...|4a3ca9315b744ce9f...|2017-11-29 22:38:47| 99.0|        13.71|2017| 11| 29| 22| 38| 47|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpyfPYfkCvdO"
      },
      "source": [
        "So far we are talking about filtering rows. How should we filter the dataframe by columns, e.g. choose a subset of columns while keep every row?\n",
        "\n",
        "Column filtering can be done by the `select` interface. If we only want to keep `order_id`, `price`, and `Y`, we can `select` in the following way:\n",
        "\n",
        "```\n",
        "df_user1.select('order_id', 'price', 'Y')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T04:28:28.807843Z",
          "start_time": "2020-11-09T04:28:28.649098Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MDCkSHMCvdO",
        "outputId": "47bd6f5a-8864-4341-cb66-7a834badda6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+----+\n",
            "|            order_id|price|   Y|\n",
            "+--------------------+-----+----+\n",
            "|00010242fe8c5a6d1...| 58.9|2017|\n",
            "|00018f77f2f0320c5...|239.9|2017|\n",
            "|000229ec398224ef6...|199.0|2018|\n",
            "+--------------------+-----+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T04:16:17.215136Z",
          "start_time": "2020-11-09T04:16:17.210901Z"
        },
        "id": "pC_o3mRECvdO"
      },
      "source": [
        "## Exercise:\n",
        "Try to filter the dataframe whose `order_id` starts with `2002`, and only keep the `order_id` and `price` columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T04:31:49.389463Z",
          "start_time": "2020-11-09T04:31:49.378451Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBBqL4CYCvdO",
        "outputId": "42bc4007-19d3-43ee-8f1b-cf9fde93086d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            order_id|price|\n",
            "+--------------------+-----+\n",
            "|20020569f2b852ebd...| 89.9|\n",
            "|2002527d89799a0cf...|78.85|\n",
            "|2002527d89799a0cf...|174.0|\n",
            "|2002ea16e75277eaa...| 15.0|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By5XYGChCvdO"
      },
      "source": [
        "## Transform\n",
        "\n",
        "We have shown how to add columns using the `withColumn` interface previousely. Now let's have a look other transforms.\n",
        "\n",
        "The simplest transform is mathematic operations, e.g. addition, subtraction, multiplication, division, etc. Check [Spark documentation](https://spark.apache.org/docs/2.3.0/api/sql/index.html)  to learn more about available math functions.\n",
        "For example, if we want to convert the `price` to use `thousand` dollar as its unit, we can\n",
        "\n",
        "```\n",
        "df_user.withColumn('price_in_k', df_user.price/1000).show()\n",
        "```\n",
        "\n",
        "We could also combine two columns information together. For example, we can get the `overall_price` by adding/mulitpling `price` and `freight_value` together.\n",
        "\n",
        "```\n",
        "df_user.withColumn('overall_price', df_user.price + df_user.freight_value).show()\n",
        "```\n",
        "```\n",
        "df_user.withColumn('overall_price', df_user.price * df_user.freight_value).show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj1oxlJ7CvdP",
        "outputId": "87756b1e-20b8-4d93-fe34-99b5d93ce22b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_user.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T04:39:37.468172Z",
          "start_time": "2020-11-09T04:39:37.099899Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le32DMMqCvdP",
        "outputId": "4d93ec5b-a26d-4654-ad65-67090b7b1fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----------+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|price_in_k|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----------+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|    0.0589|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|    0.2399|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|     0.199|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|   0.01299|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|    0.1999|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_user.withColumn('price_in_k', df_user.price/1000).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T04:39:37.569665Z",
          "start_time": "2020-11-09T04:39:37.470630Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HWo-uctCvdP",
        "outputId": "70febc3a-5e95-49b1-a45f-897704e46124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+-----------+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|Total_price|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+-----------+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|    782.781|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|   4781.207|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|    3556.13|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|   166.1421|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|   3626.186|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_user.withColumn('Total_price', df_user.price * df_user.freight_value).show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuSj6fKWCvdP"
      },
      "outputs": [],
      "source": [
        "df_swe=df_user.withColumn('overall_price', df_user.price + df_user.freight_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHeGZWNQCvdQ",
        "outputId": "3d2a4182-98a0-4f6f-8bae-595ff94469cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+------------------+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|     overall_price|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+------------------+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35|  58.9|        13.29|             72.19|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13| 239.9|        19.93|            259.83|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30| 199.0|        17.87|            216.87|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18| 12.99|        12.79|             25.78|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51| 199.9|        18.14|218.04000000000002|\n",
            "|00048cc3ae777c65d...|            1|ef92defde845ab845...|6426d21aca402a131...|2017-05-23 03:55:27|  21.9|        12.69|34.589999999999996|\n",
            "|00054e8431b9d7675...|            1|8d4f2bb7e93e6710a...|7040e82f899a04d1b...|2017-12-14 12:10:31|  19.9|        11.85|             31.75|\n",
            "|000576fe39319847c...|            1|557d850972a7d6f79...|5996cddab893a4652...|2018-07-10 12:30:45| 810.0|        70.75|            880.75|\n",
            "|0005a1a1728c9d785...|            1|310ae3c140ff94b03...|a416b6a846a117243...|2018-03-26 18:31:29|145.95|        11.65|             157.6|\n",
            "|0005f50442cb953dc...|            1|4535b0e1091c278df...|ba143b05f0110f0dc...|2018-07-06 14:10:56| 53.99|         11.4|             65.39|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_swe.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o644MosbCEn1"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Find the square root of each order's price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T04:44:10.187926Z",
          "start_time": "2020-11-09T04:44:10.178481Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUu0qiRcCvdQ",
        "outputId": "b6a66855-d8ba-4efa-8bd8-7e83cf3435ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+------------------+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|        sqrt_price|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+------------------+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29| 7.674633541739957|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|15.488705562441298|\n",
            "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|14.106735979665885|\n",
            "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|3.6041642581880198|\n",
            "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14| 14.13859964777276|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSE7Cd-jCvdR"
      },
      "source": [
        "Remember Spark does not allow in-place transform, so each of our transform needs to be expressed by adding a new column into the dataframe. The good thing is that we can always use the `coloumn` filtering to keep only the columns we want.\n",
        "\n",
        "It means you cannot do transformation on a current column in the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WekQM4SZCvdR"
      },
      "source": [
        "## Aggregation\n",
        "\n",
        "Aggregation is a special `transform`: Instead of altering cells on a per-row basis, aggregation works with a group of rows and generate new values from each group. Therefore, aggregation is always used together with the `groupby` operation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12Kh4rS0Eb7b",
        "outputId": "4fa45af1-4eb8-484d-be47-03852bcce4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|   Y|  M|  D|  H|Min|  S|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|2017|  9| 19|  9| 45| 35|\n",
            "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|2017|  5|  3| 11|  5| 13|\n",
            "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----+---+---+---+---+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df_user1.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T06:17:31.986475Z",
          "start_time": "2020-11-09T06:17:30.581883Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOa9bE05CvdR",
        "outputId": "0af15a24-49e6-48a0-a02d-8c558ec937ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+------------------+\n",
            "|   Y|  M|        sum(price)|\n",
            "+----+---+------------------+\n",
            "|2017|  3|343242.59999999945|\n",
            "|2017|  8| 560093.3400000128|\n",
            "|2017| 10| 658020.2300000164|\n",
            "|2018|  1| 845279.1800000344|\n",
            "|2018|  3|1030527.8400000384|\n",
            "+----+---+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum, count\n",
        "\n",
        "df_user1.groupby('Y',\"M\").agg(sum('price')).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34quzssQCvdR"
      },
      "source": [
        "Our new column's name is \"sum(price)\", which is not convenient to use later. We could use `alias` to give the column a better name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T06:17:04.300312Z",
          "start_time": "2020-11-09T06:17:02.709564Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85qXqvb6CvdS",
        "outputId": "64bf4f9d-fcc1-418d-ca1d-7ed7a185cfcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+------------------+\n",
            "|   Y|  M|      Yearly_total|\n",
            "+----+---+------------------+\n",
            "|2017|  3|343242.59999999945|\n",
            "|2017|  8| 560093.3400000128|\n",
            "|2017| 10| 658020.2300000164|\n",
            "|2018|  1| 845279.1800000344|\n",
            "|2018|  3|1030527.8400000384|\n",
            "+----+---+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_user1.groupby('Y', 'M').agg(\n",
        "    sum('price').alias('Yearly_total')\n",
        ").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T06:20:30.444334Z",
          "start_time": "2020-11-09T06:20:30.436343Z"
        },
        "id": "Tri0MvzBCvdS"
      },
      "source": [
        "It is also possible to have multiple aggregation with each group. For example, we can get monthly price and monthly count using the following call\n",
        "\n",
        "```\n",
        "df_user1.groupby('Y', 'M').agg(\n",
        "    sum('price').alias('monthly_total'),\n",
        "    count('price').alias('montyly_count'),\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T06:20:44.601649Z",
          "start_time": "2020-11-09T06:20:43.068515Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVUP0zy6CvdT",
        "outputId": "ef7d1f63-9dfd-49b9-d1c1-cbaac0bee183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+------------------+-------------+\n",
            "|   Y|  M|     monthly_total|montyly_count|\n",
            "+----+---+------------------+-------------+\n",
            "|2017|  3|343242.59999999945|         2751|\n",
            "|2017|  8| 560093.3400000128|         5042|\n",
            "|2017| 10| 658020.2300000164|         5189|\n",
            "|2018|  1| 845279.1800000344|         7492|\n",
            "|2018|  3|1030527.8400000384|         8759|\n",
            "+----+---+------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_user1.groupby('Y', 'M').agg(\n",
        "    sum('price').alias('monthly_total'),\n",
        "    count('price').alias('montyly_count'),\n",
        ").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMFxadcQCvdT"
      },
      "source": [
        "You may notice we use the `show` interface to have a quick look at the dataframe content. However,`show` does not do full operations on the dataframe. If we want to apply the defined operations fully to the dataframe, we need to use `triggers`. The most frequently used trigger is `count`, which tell us how many records we have in a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T07:36:35.136273Z",
          "start_time": "2020-11-09T07:36:32.978154Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92XmfooDCvdT",
        "outputId": "97a4194e-46fc-41e7-f0b8-9e576a324c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "df_agg = df_user1.groupby('Y', 'M').agg(\n",
        "    sum('price').alias('monthly_total'),\n",
        "    count('price').alias('monthly_count'),\n",
        ")\n",
        "\n",
        "print(df_agg.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdxMQ-78CvdU"
      },
      "source": [
        "Another useful trigger is `collect`, which pulls data out of executors and save to the driver. For examples, we may want to have the full content of `df_agg` to see the monthly trend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T07:36:38.734010Z",
          "start_time": "2020-11-09T07:36:36.820414Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR2m95r5CvdU",
        "outputId": "6c5a4df3-3b45-42c4-b66d-46fbc55ef071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "agg_result = df_agg.collect()\n",
        "print(len(agg_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7qP0MsZCvdU"
      },
      "source": [
        "The data we collect back are wrapped in the `Row` class. The `Row` class makes it easier for us to address individual fields in each record."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T07:36:41.435386Z",
          "start_time": "2020-11-09T07:36:41.430481Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WemIt6ypCvdU",
        "outputId": "462eef68-ecd4-4765-991e-57e8d4ba8263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of the object: <class 'pyspark.sql.types.Row'>\n",
            "Row(Y=2017, M=3, monthly_total=343242.59999999945, monthly_count=2751)\n",
            "2017 3 343242.59999999945 2751\n"
          ]
        }
      ],
      "source": [
        "for i in agg_result:\n",
        "    print(\"Type of the object: {}\".format(type(i)))\n",
        "    print(i)\n",
        "    print(i.Y, i.M, i.monthly_total, i.monthly_count)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJCIpI8lCvdU"
      },
      "source": [
        "There is also an convinient function to convert a Spark dataframe into a Pandas dataframe, `to_pandas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T07:51:13.992644Z",
          "start_time": "2020-11-09T07:51:10.790520Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "oDUPthC3CvdV",
        "outputId": "50fe53fe-deec-4c62-c196-ca3a60bfc9f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='ym'>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pandas_df = df_agg.orderBy('Y', 'M').toPandas()\n",
        "pandas_df['ym'] = pandas_df['Y'].apply(lambda x: '{:04d}'.format(x)) + '-' + pandas_df['M'].apply(lambda x: '{:02d}'.format(x))\n",
        "pandas_df.plot(x='ym', y='monthly_total')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7hB0QACvdV"
      },
      "source": [
        "## Load\n",
        "\n",
        "The `Load` operation refers to save the processed data in some persistent storage engines, e.g. relational databases, key-value stores, object stores, and file systems.\n",
        "\n",
        "Spark supports write data to most of these popular engines:\n",
        "* MySQL\n",
        "* Postgresql\n",
        "* Elasticserch\n",
        "* Cassandra\n",
        "* Kafka\n",
        "* AWS S3\n",
        "* Azure Blob\n",
        "* HDFS\n",
        "* Alluxio\n",
        "\n",
        "Let's take writing to csv files as an example.\n",
        "```\n",
        "df_agg.repartition(1).write.mode('overwrite').option('header', 'true').csv('/home/jovyan/output')\n",
        "```\n",
        "where\n",
        "* `repartition(1)` is used to reduce number of output files to 1\n",
        "* `mode('overwrite')` tells Spark to delete any exisiting data in the output directory\n",
        "* `option('header', 'true')` controls if a header line should be added to the csv files\n",
        "\n",
        "You can try to modify the arguments to observe Spark's writting behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T08:03:18.901660Z",
          "start_time": "2020-11-09T08:03:16.022199Z"
        },
        "id": "ffFF26maCvdV"
      },
      "outputs": [],
      "source": [
        "df_agg.repartition(1).write.mode('overwrite').option('header', 'true').csv('output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-09T08:07:03.236693Z",
          "start_time": "2020-11-09T08:07:02.411310Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEnuepGZCvdV",
        "outputId": "23da7bbe-0ad5-49a7-c0f8-3f977b0dd11d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Y,M,monthly_total,monthly_count\n",
            "2017,3,343242.59999999945,2751\n",
            "2017,8,560093.3400000128,5042\n",
            "2017,10,658020.2300000164,5189\n",
            "2018,1,845279.1800000344,7492\n",
            "2018,3,1030527.8400000384,8759\n",
            "2018,8,1072147.7000000272,8815\n",
            "2017,7,465282.10000000865,4116\n",
            "2016,9,194.47,4\n",
            "2018,5,1084358.550000023,8765\n",
            "2016,10,49580.55000000016,365\n",
            "2017,12,898920.9800000272,7726\n",
            "2020,4,199.98,2\n",
            "2017,9,617046.1200000154,4724\n",
            "2016,12,10.9,1\n",
            "2017,4,308148.3199999984,2364\n",
            "2018,9,14502.499999999987,99\n",
            "2018,7,811132.0800000194,6672\n",
            "2020,2,145.98,2\n",
            "2018,6,870736.8400000221,6897\n",
            "2017,2,245982.00999999608,1866\n",
            "2017,11,883351.6300000291,7355\n",
            "2017,5,505655.46000000933,4150\n",
            "2018,2,819228.8500000269,7375\n",
            "2017,6,469001.05000000703,3801\n",
            "2017,1,80124.73999999998,681\n",
            "2018,4,958729.7000000284,7637\n"
          ]
        }
      ],
      "source": [
        "!cat output/*.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06baF8ctPfyU"
      },
      "source": [
        "### **Trying it out**\n",
        "\n",
        "Follow the questions stated below to find out some of the common commands that we use for data cleaning and processing that is different from Pandas\n",
        "\n",
        "[List to functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me7UHc_gPiWS"
      },
      "source": [
        "How do we determine is there any null(Empty) values in the dataframe?\n",
        "\n",
        "[Click Me](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.isnull.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxPmwZwEPhVn",
        "outputId": "a9c988ac-2471-4098-f8a6-2fd871032858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
            "|order_id|order_item_id|product_id|seller_id|shipping_limit_date|price|freight_value|\n",
            "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
            "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "#filter data by null values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsGAu0wMRgcN"
      },
      "source": [
        "Given that it can only check one by one as shown above. How can we check all the columns?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkF3uz06P36R",
        "outputId": "4c788d0f-d12e-4aaa-d766-5037653241d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'order_id' has 0 null values\n",
            "Column 'order_item_id' has 0 null values\n",
            "Column 'product_id' has 0 null values\n",
            "Column 'seller_id' has 0 null values\n",
            "Column 'shipping_limit_date' has 0 null values\n",
            "Column 'price' has 0 null values\n",
            "Column 'freight_value' has 0 null values\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQzgTZW9Ru5S"
      },
      "source": [
        "If there is any null values is there a way to drop it?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMNHr8MuRtF5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTbPPI8TTtk7"
      },
      "source": [
        "How can I check the shape of my dataframe? (Pandas `.shape()` )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lE2FXRuTRxv",
        "outputId": "14c42257-216d-4ea3-b079-7a37ed28deb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of DataFrame: (112650, 7)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwSPDBu3WhXF"
      },
      "source": [
        "Determine the spread of the data using\n",
        "\n",
        "```\n",
        ".summary()\n",
        "```\n",
        "\n",
        "```\n",
        ".descirbe()\n",
        "```\n",
        "\n",
        "What is the difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYwKhzj7Wga3",
        "outputId": "c2f824df-7fee-4b9d-a791-aaedfbbe9830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+------------------+--------------------+--------------------+-------------------+------------------+------------------+\n",
            "|summary|            order_id|     order_item_id|          product_id|           seller_id|shipping_limit_date|             price|     freight_value|\n",
            "+-------+--------------------+------------------+--------------------+--------------------+-------------------+------------------+------------------+\n",
            "|  count|              112650|            112650|              112650|              112650|             112650|            112650|            112650|\n",
            "|   mean|                NULL|1.1978339991122948|                NULL|                NULL|               NULL|120.65373901477311| 19.99031992898562|\n",
            "| stddev|                NULL|0.7051240313951734|                NULL|                NULL|               NULL| 183.6339280502597|15.806405412296998|\n",
            "|    min|00010242fe8c5a6d1...|                 1|00066f42aeeb9f300...|0015a82c2db000af6...|2016-09-19 00:15:34|              0.85|              0.00|\n",
            "|    max|fffe41c64501cc87c...|                 9|fffe9eeff12fcbd74...|ffff564a4f9085cd2...|2020-04-09 22:35:08|            999.99|             99.97|\n",
            "+-------+--------------------+------------------+--------------------+--------------------+-------------------+------------------+------------------+\n",
            "\n",
            "+-------+--------------------+------------------+--------------------+--------------------+-------------------+------------------+------------------+\n",
            "|summary|            order_id|     order_item_id|          product_id|           seller_id|shipping_limit_date|             price|     freight_value|\n",
            "+-------+--------------------+------------------+--------------------+--------------------+-------------------+------------------+------------------+\n",
            "|  count|              112650|            112650|              112650|              112650|             112650|            112650|            112650|\n",
            "|   mean|                NULL|1.1978339991122948|                NULL|                NULL|               NULL|120.65373901477311| 19.99031992898562|\n",
            "| stddev|                NULL|0.7051240313951734|                NULL|                NULL|               NULL| 183.6339280502597|15.806405412296998|\n",
            "|    min|00010242fe8c5a6d1...|                 1|00066f42aeeb9f300...|0015a82c2db000af6...|2016-09-19 00:15:34|              0.85|              0.00|\n",
            "|    25%|                NULL|               1.0|                NULL|                NULL|               NULL|              39.9|             13.08|\n",
            "|    50%|                NULL|               1.0|                NULL|                NULL|               NULL|             74.99|             16.26|\n",
            "|    75%|                NULL|               1.0|                NULL|                NULL|               NULL|             134.9|             21.15|\n",
            "|    max|fffe41c64501cc87c...|                 9|fffe9eeff12fcbd74...|ffff564a4f9085cd2...|2020-04-09 22:35:08|            999.99|             99.97|\n",
            "+-------+--------------------+------------------+--------------------+--------------------+-------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCfj0AUqXsvQ"
      },
      "source": [
        "The boss would like to determine which product has sold the most?\n",
        "\n",
        "How can we do that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O69JtdTAXHdQ",
        "outputId": "08ab1542-8961-4d25-9156-285cdacffc57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|          product_id|count|\n",
            "+--------------------+-----+\n",
            "|aca2eb7d00ea1a7b8...|  527|\n",
            "|99a4788cb24856965...|  488|\n",
            "|422879e10f4668299...|  484|\n",
            "|389d119b48cf3043d...|  392|\n",
            "|368c6c730842d7801...|  388|\n",
            "|53759a2ecddad2bb8...|  373|\n",
            "|d1c427060a0f73f6b...|  343|\n",
            "|53b36df67ebb7c415...|  323|\n",
            "|154e7e31ebfa09220...|  281|\n",
            "|3dd2a17168ec895c7...|  274|\n",
            "|2b4609f8948be1887...|  260|\n",
            "|7c1bd920dbdf22470...|  231|\n",
            "|a62e25e09e05e6faf...|  226|\n",
            "|5a848e4ab52fd5445...|  197|\n",
            "|bb50f2e236e5eea01...|  195|\n",
            "|e0d64dcfaa3b6db5c...|  194|\n",
            "|e53e557d5a159f5aa...|  183|\n",
            "|42a2c92a0979a949c...|  183|\n",
            "|b532349fe46b38fbc...|  169|\n",
            "|35afc973633aaeb6b...|  165|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieE4qGfrYV0X"
      },
      "source": [
        "How do we find the top 10 items only?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzBUkJ5PYgsu",
        "outputId": "833261eb-dbf6-442d-fd25-3a07fa8798b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|          product_id|count|\n",
            "+--------------------+-----+\n",
            "|aca2eb7d00ea1a7b8...|  527|\n",
            "|99a4788cb24856965...|  488|\n",
            "|422879e10f4668299...|  484|\n",
            "|389d119b48cf3043d...|  392|\n",
            "|368c6c730842d7801...|  388|\n",
            "|53759a2ecddad2bb8...|  373|\n",
            "|d1c427060a0f73f6b...|  343|\n",
            "|53b36df67ebb7c415...|  323|\n",
            "|154e7e31ebfa09220...|  281|\n",
            "|3dd2a17168ec895c7...|  274|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTXj5PerayvN"
      },
      "source": [
        "How do we determine the total amount earned for the top three items?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TviF29YXaVY8",
        "outputId": "709f7e2b-3cf5-4c70-ef93-3690e18acb84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['aca2eb7d00ea1a7b8ebd4e68314663af',\n",
              " '99a4788cb24856965c36a24e339b6058',\n",
              " '422879e10f46682990de24d770e7f83d']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7qRjiXoa_Ai",
        "outputId": "ef59b659-bbbf-4279-c1c6-bf764a913983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of DataFrame: (1499, 7)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "shfMQxgQbJ4n",
        "outputId": "8cb2c2be-1488-48f6-89aa-932bd3b74708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|        sum(price)|\n",
            "+------------------+\n",
            "|107211.67999999788|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heWNQLQxYoOr"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Y3S1_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
